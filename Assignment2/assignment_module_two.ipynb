{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f81943",
   "metadata": {},
   "source": [
    "Davide Ligabue,   MATR: 0001191187, davide.ligabue@studio.unibo.it  \n",
    "Leonardo Benini,  MATR: 0001189330, leonardo.benini3@studio.unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41535fd",
   "metadata": {
    "id": "d41535fd"
   },
   "source": [
    "# Assignment Module 2: Pet Classification\n",
    "\n",
    "The goal of this assignment is to implement a neural network that classifies images of 37 breeds of cats and dogs from the [Oxford-IIIT-Pet dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/). The assignment is divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1476550",
   "metadata": {
    "id": "b1476550"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The following cells contain the code to download and access the dataset you will be using in this assignment. Note that, although this dataset features each and every image from [Oxford-IIIT-Pet](https://www.robots.ox.ac.uk/~vgg/data/pets/), it uses a different train-val-test split than the original authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91101a0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28101,
     "status": "ok",
     "timestamp": 1752505854975,
     "user": {
      "displayName": "Davide Ligabue",
      "userId": "11892369227096187309"
     },
     "user_tz": -120
    },
    "id": "91101a0d",
    "outputId": "0d4edff4-6095-4725-eb9d-5ad4010bcc76"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/CVLAB-Unibo/ipcv-assignment-2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8fb0d2",
   "metadata": {
    "executionInfo": {
     "elapsed": 3908,
     "status": "ok",
     "timestamp": 1752505858887,
     "user": {
      "displayName": "Davide Ligabue",
      "userId": "11892369227096187309"
     },
     "user_tz": -120
    },
    "id": "0d8fb0d2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c9929",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1752505858897,
     "user": {
      "displayName": "Davide Ligabue",
      "userId": "11892369227096187309"
     },
     "user_tz": -120
    },
    "id": "b99c9929"
   },
   "outputs": [],
   "source": [
    "class OxfordPetDataset(Dataset):\n",
    "    def __init__(self, split: str, transform=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = Path(\"ipcv-assignment-2\") / \"dataset\"\n",
    "        self.split = split\n",
    "        self.names, self.labels = self._get_names_and_labels()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n",
    "        img_path = self.root / \"images\" / f\"{self.names[idx]}.jpg\"\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def get_num_classes(self) -> int:\n",
    "        return max(self.labels) + 1\n",
    "\n",
    "    def _get_names_and_labels(self) -> Tuple[List[str], List[int]]:\n",
    "        names = []\n",
    "        labels = []\n",
    "\n",
    "        with open(self.root / \"annotations\" / f\"{self.split}.txt\") as f:\n",
    "            for line in f:\n",
    "                name, label = line.replace(\"\\n\", \"\").split(\" \")\n",
    "                names.append(name),\n",
    "                labels.append(int(label) - 1)\n",
    "\n",
    "        return names, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e655bd",
   "metadata": {
    "id": "b4e655bd"
   },
   "source": [
    "## Part 1: design your own network\n",
    "\n",
    "Your goal is to implement a convolutional neural network for image classification and train it from scratch on `OxfordPetDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the test split of ~60%. You are free to achieve this however you want, except for a few rules you must follow:\n",
    "\n",
    "- Compile this notebook by displaying the results obtained by the best model you found throughout your experimentation; then show how, by removing some of its components, its performance drops. In other words, do an *ablation study* to prove that your design choices have a positive impact on the final result.\n",
    "\n",
    "- Do not instantiate an off-the-self PyTorch network. Instead, construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you cannot use e.g. `torchvision.models.alexnet`.\n",
    "\n",
    "- Show your results and ablations with plots, tables, images, etc. â€” the clearer, the better.\n",
    "\n",
    "Don't be too concerned with your model performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded more points than a poorly experimentally validated model with higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0c8f42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 99,
     "status": "ok",
     "timestamp": 1752505863818,
     "user": {
      "displayName": "Davide Ligabue",
      "userId": "11892369227096187309"
     },
     "user_tz": -120
    },
    "id": "ec0c8f42",
    "outputId": "00aec9e3-37b8-4201-c6c3-c18134109742"
   },
   "outputs": [],
   "source": [
    "# ImageNet mean and std for normalization\n",
    "IMG_SIZE = (224, 224) # A common size for image classification tasks\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.TrivialAugmentWide(),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD)\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD)\n",
    "])\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = OxfordPetDataset(split=\"train\", transform=train_transform)\n",
    "val_dataset = OxfordPetDataset(split=\"val\", transform=val_test_transform)\n",
    "test_dataset = OxfordPetDataset(split=\"test\", transform=val_test_transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "BATCH_SIZE = 128 # You can tune this hyperparameter\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Get number of classes\n",
    "NUM_CLASSES = train_dataset.get_num_classes()\n",
    "INPUT_DIM = len(train_dataset[0][0])\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fb1dad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1752505900533,
     "user": {
      "displayName": "Davide Ligabue",
      "userId": "11892369227096187309"
     },
     "user_tz": -120
    },
    "id": "99fb1dad",
    "outputId": "c0170bbd-2421-404e-94f0-1bde3fa5e41c"
   },
   "outputs": [],
   "source": [
    "# Check for CUDA availability\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "fix_random(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MZKbhBm_cgg8",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1752505902530,
     "user": {
      "displayName": "Davide Ligabue",
      "userId": "11892369227096187309"
     },
     "user_tz": -120
    },
    "id": "MZKbhBm_cgg8"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_classes: int,\n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 n_hidden_layers: int,\n",
    "                 use_batchnorm: bool,\n",
    "                 dropout_p: float,\n",
    "                 stem_kernel_size: int):\n",
    "        super().__init__()\n",
    "        padding = (stem_kernel_size - 1) // 2\n",
    "\n",
    "        # Stem layer\n",
    "        stem_layers = [nn.Conv2d(input_dim, hidden_dim, kernel_size=stem_kernel_size, stride=2, padding=padding, bias=not use_batchnorm)]\n",
    "        if use_batchnorm: stem_layers.append(nn.BatchNorm2d(hidden_dim))\n",
    "        stem_layers.extend([nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2)])\n",
    "        self.stem = nn.Sequential(*stem_layers)\n",
    "\n",
    "        # Convolutional blocks\n",
    "        conv_blocks = []\n",
    "        in_channels = hidden_dim\n",
    "        for _ in range(n_hidden_layers):\n",
    "            out_channels = in_channels * 2\n",
    "            block = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=not use_batchnorm)]\n",
    "            if use_batchnorm: block.append(nn.BatchNorm2d(out_channels))\n",
    "            block.extend([nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2)])\n",
    "            conv_blocks.append(nn.Sequential(*block))\n",
    "            in_channels = out_channels\n",
    "        self.features = nn.Sequential(*conv_blocks)\n",
    "\n",
    "        # FC Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(), nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(in_channels, max(in_channels // 2, 1), bias=False),\n",
    "            nn.ReLU(inplace=True), nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(max(in_channels // 2, 1), n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KunJ22wLdmVQ",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1752505905729,
     "user": {
      "displayName": "Davide Ligabue",
      "userId": "11892369227096187309"
     },
     "user_tz": -120
    },
    "id": "KunJ22wLdmVQ"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model: nn.Module,\n",
    "                dataloader: DataLoader,\n",
    "                criterion: nn.Module,\n",
    "                optimizer: optim.Optimizer,\n",
    "                device: torch.device,\n",
    "                scheduler: Optional[lr_scheduler.LRScheduler] = None) -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None: \n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / total_samples\n",
    "    avg_acc = correct_predictions.double() / total_samples\n",
    "    return avg_loss, avg_acc.item()\n",
    "\n",
    "def evaluate_model(model: nn.Module,\n",
    "                   dataloader: DataLoader,\n",
    "                   criterion: nn.Module,\n",
    "                   device: torch.device) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels.data)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / total_samples\n",
    "    avg_acc = correct_predictions.double() / total_samples\n",
    "    return avg_loss, avg_acc.item()\n",
    "\n",
    "def plot_history(history: Dict[str, List[float]]):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4725fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "HIDDEN_DIM = 64\n",
    "\n",
    "def run_experiment(config: Dict, model = None):\n",
    "    print(\"=\"*60 + f\"\\nStart running the model: {config['experiment_name']}\\n\" + \"=\"*60)\n",
    "    fix_random(42)\n",
    "\n",
    "    if model is None: \n",
    "        model = CustomModel(input_dim=INPUT_DIM,\n",
    "                            hidden_dim=HIDDEN_DIM,\n",
    "                            n_classes=NUM_CLASSES,\n",
    "                            n_hidden_layers=config['n_layers'],\n",
    "                            use_batchnorm=config['use_batchnorm'],\n",
    "                            dropout_p=config['dropout_p'],\n",
    "                            stem_kernel_size=config['stem_kernel_size']\n",
    "                            ).to(DEVICE)\n",
    "\n",
    "    # Setup of Early stopping, LR schedule and weight decay\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.get(\"lr\", 1e-3), weight_decay=config['weight_decay'])\n",
    "    if config['use_scheduler']:\n",
    "        warmup_scheduler = lr_scheduler.LinearLR(optimizer, start_factor=0.01, end_factor=1.0, total_iters=5)\n",
    "        main_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=15)\n",
    "\n",
    "    # Training Loop\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_val_acc = 0.0\n",
    "    pbar = tqdm(range(EPOCHS), desc=f\"Training {config['experiment_name']}\")\n",
    "    for epoch in pbar:\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, DEVICE)\n",
    "        history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n",
    "        if config['use_scheduler']:\n",
    "            if epoch < 5: warmup_scheduler.step()\n",
    "            else: main_scheduler.step(val_acc)\n",
    "        if val_acc > best_val_acc: \n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f\"{config['experiment_name']}.pt\")\n",
    "            #print(f\"Saved new best model, val acc: {val_acc}\")\n",
    "        pbar.set_postfix({\"Val Acc\": f\"{val_acc:.4f}\", \"Train Acc\": f\"{train_acc:.4f}\"})\n",
    "\n",
    "    # Final test and save result\n",
    "    sd = torch.load(f\"{config['experiment_name']}.pt\")\n",
    "    model.load_state_dict(sd)\n",
    "    _, test_acc = evaluate_model(model, test_loader, criterion, DEVICE)\n",
    "    print(f\"Model '{config['experiment_name']}' completed. Test Accuracy: {test_acc:.4f}\\n\")\n",
    "\n",
    "    result = config.copy()\n",
    "    result.update({'test_accuracy': test_acc, 'best_val_accuracy': best_val_acc, 'history': history})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F8rMTg2pcg7l",
   "metadata": {
    "id": "F8rMTg2pcg7l"
   },
   "source": [
    "### Ablation study\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a0c556",
   "metadata": {},
   "source": [
    "Model's description\n",
    "\n",
    "The following is the description of how our model was originally thought. A complete ablation study was then made on this model, both on its parameters and on different training strategies (such as the learning rate scheduler or weight decay). After the ablation study the optimal choice of model is presented.\n",
    "\n",
    "Our custom model is very simple in its structure but it has been able to achive good performance (around 75\\% accuracy on the test set). The following is the general structure of the architecture:\n",
    "- Stem layer as first layer to reduce spatial size: the study explores how changing the kernel's size of this first convolution influences the tuning performance\n",
    "- A batch norm layer, added for training stability\n",
    "- Finally a max pooling layer, to further reduce the spatial dimensions, but also for introducing invariance from the input.\n",
    "\n",
    "Following the first layer a stack of convolutional blocks is added, this choice was made for a better understanding of the model and for an easier implementation (inspired from the AlexNet architecture). Each convolutional block doubles the number of channels in order to increase the complexity of the feature we are looking for (an idea that had already been formulated at the time of LeNet5). \n",
    "Each block is made of:\n",
    "- $3\\times 3$ kernels with padding $=1$ and with no stride\n",
    "- Batch normalization layer\n",
    "- ReLU\n",
    "- Max pooling layer\n",
    "\n",
    "The last part of the architecture, i.e. the classifier, is composed of:\n",
    "- Adaptive avegering pooling layer, which removes the spatial dimensions without having to know the exact input size\n",
    "- Flattening\n",
    "- Dropout layer, to reduce overfitting\n",
    "- Linear layer to reduce the dimension of the feature vector before the ReLU layer\n",
    "- ReLU\n",
    "- Dropout layer\n",
    "- A final linear layer which classifies the feature vectors in one of the given classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U8hH9SqrdwIZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "716c6c19df33489f8f170ab7d94b47f0",
      "4ebbdf9d14b3474da7a106cbf3d47029",
      "596c2d66539b4cfd89dee416ab7152fa",
      "b062e13508974a5e8a06366f24f1094b",
      "462cccdddece4767b6780d5cda70b386",
      "b50a3ddb0908475297792e67f2c8e82b",
      "c1b8ec3868be4727ada3ebf9ff70193c",
      "67fc32d31aa74c1186b42ab1a0f09b3e",
      "602e8996a79c4592928f043cf1a7a155",
      "7a4795c70250490882ef10c85fb07d65",
      "5bb6a216519842cdbaa740e786dfe74d"
     ]
    },
    "id": "U8hH9SqrdwIZ",
    "outputId": "c82ef4fa-ff1c-4e1c-a5f0-2aa8b5b379af"
   },
   "outputs": [],
   "source": [
    "# Ablation study on model's configuration\n",
    "ablation_configs = [\n",
    "    # Baseline\n",
    "    {\"experiment_name\": \"Baseline (4 Layers, Kernel 11)\", \"n_layers\": 4, \"stem_kernel_size\": 11, \"use_batchnorm\": True, \"dropout_p\": 0.5, \"weight_decay\": 1e-3, \"use_scheduler\": True},\n",
    "\n",
    "    # Ablation on the model's components\n",
    "    {\"experiment_name\": \"No BatchNorm\", \"n_layers\": 4, \"stem_kernel_size\": 11, \"use_batchnorm\": False, \"dropout_p\": 0.5, \"weight_decay\": 1e-3, \"use_scheduler\": True},\n",
    "    {\"experiment_name\": \"No Dropout\", \"n_layers\": 4, \"stem_kernel_size\": 11, \"use_batchnorm\": True, \"dropout_p\": 0.0, \"weight_decay\": 1e-3, \"use_scheduler\": True},\n",
    "    {\"experiment_name\": \"No Weight Decay\", \"n_layers\": 4, \"stem_kernel_size\": 11, \"use_batchnorm\": True, \"dropout_p\": 0.5, \"weight_decay\": 0.0, \"use_scheduler\": True},\n",
    "    {\"experiment_name\": \"No LR Scheduler\", \"n_layers\": 4, \"stem_kernel_size\": 11, \"use_batchnorm\": True, \"dropout_p\": 0.5, \"weight_decay\": 1e-3, \"use_scheduler\": False},\n",
    "\n",
    "    # Ablation on the number of Hidden Layers\n",
    "    {\"experiment_name\": \"2 Hidden Layers\", \"n_layers\": 2, \"stem_kernel_size\": 11, \"use_batchnorm\": True, \"dropout_p\": 0.5, \"weight_decay\": 1e-3, \"use_scheduler\": True},\n",
    "    {\"experiment_name\": \"3 Hidden Layers\", \"n_layers\": 3, \"stem_kernel_size\": 11, \"use_batchnorm\": True, \"dropout_p\": 0.5, \"weight_decay\": 1e-3, \"use_scheduler\": True},\n",
    "    {\"experiment_name\": \"5 Hidden Layers\", \"n_layers\": 5, \"stem_kernel_size\": 11, \"use_batchnorm\": True, \"dropout_p\": 0.5, \"weight_decay\": 1e-3, \"use_scheduler\": True},\n",
    "    \n",
    "    # Ablation on the dimension of the kernel in the stem layer\n",
    "    {\"experiment_name\": \"Stem Kernel 7x7\", \"n_layers\": 4, \"stem_kernel_size\": 7, \"use_batchnorm\": True, \"dropout_p\": 0.5, \"weight_decay\": 1e-3, \"use_scheduler\": True},\n",
    "    {\"experiment_name\": \"Stem Kernel 3x3\", \"n_layers\": 4, \"stem_kernel_size\": 5, \"use_batchnorm\": True, \"dropout_p\": 0.5, \"weight_decay\": 1e-3, \"use_scheduler\": True},\n",
    "]\n",
    "\n",
    "all_results = [run_experiment(config) for config in ablation_configs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_key_histories(df, experiments_to_plot, title):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    for name in experiments_to_plot:\n",
    "        row = df[df['experiment_name'] == name]\n",
    "        if row.empty: continue\n",
    "        history = row.iloc[0]['history']\n",
    "        plt.plot(history['val_acc'], label=name, lw=2)\n",
    "\n",
    "    plt.title(title, fontsize=16, weight='bold')\n",
    "    plt.xlabel('Epochs', fontsize=12); plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.legend(); plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.ylim(bottom=0); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UQAJgy-cfG0j",
   "metadata": {
    "id": "UQAJgy-cfG0j"
   },
   "outputs": [],
   "source": [
    "# Create pandas dataframe with results\n",
    "df_results = pd.DataFrame(all_results).sort_values(by='test_accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Ablation study summary:\")\n",
    "display(df_results[['experiment_name', 'test_accuracy', 'best_val_accuracy', 'n_layers', 'stem_kernel_size', 'dropout_p']].set_index('experiment_name'))\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# 1. Bar plot to compare final accuracy\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='test_accuracy', y='experiment_name', data=df_results, palette='viridis', orient='h')\n",
    "plt.title(\"Ablation study: Accuracy on test set\", fontsize=16, weight='bold')\n",
    "plt.xlabel('Test accuracy', fontsize=12)\n",
    "plt.ylabel('Config', fontsize=12)\n",
    "plt.xlim(0, max(df_results['test_accuracy']) * 1.15)\n",
    "for index, value in enumerate(df_results['test_accuracy']):\n",
    "    plt.text(value + 0.005, index, f'{value:.3f}', va='center', weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Validation accuracy plot\n",
    "plot_key_histories(df_results,\n",
    "                   ['Baseline (4 Layers, Kernel 11)', \"No BatchNorm\",\"No Dropout\",\"No Weight Decay\",\"No LR Scheduler\"],\n",
    "                   \"Regularizers vs Validation accuracy\")\n",
    "\n",
    "plot_key_histories(df_results,\n",
    "                   ['Baseline (4 Layers, Kernel 11)', '2 Hidden Layers', '3 Hidden Layers', '5 Hidden Layers'],\n",
    "                   \"Number of layers vs Validation accuracy\")\n",
    "\n",
    "plot_key_histories(df_results,\n",
    "                   ['Baseline (4 Layers, Kernel 11)', 'Stem Kernel 7x7', 'Stem Kernel 3x3'],\n",
    "                   \"Kernel dimension vs Validation accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1zgZ6_mucjkF",
   "metadata": {
    "id": "1zgZ6_mucjkF"
   },
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32734253",
   "metadata": {},
   "source": [
    "As one can see from the display of the results of the ablation study the best choices are:\n",
    "- using weight decay\n",
    "- using a learning rate schedule (in our case a combination of an initial warm up phase followed by a reduction on plateau)\n",
    "- using batch norm and dropout for regularization, to avoid overfitting\n",
    "- using an initial kernel $7 \\times 7$, as it is slightly more performant than the $11 \\times 11$ baseline and uses less parameters, therefore less FLOPS and time to train\n",
    "- using $4$ convolutional blocks, the performance is marginally better than the model with $5$ hidden layers and with less parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4d9ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModel(input_dim=INPUT_DIM,\n",
    "                          hidden_dim=HIDDEN_DIM,\n",
    "                          n_classes=NUM_CLASSES,\n",
    "                          n_hidden_layers=4,\n",
    "                          use_batchnorm=True,\n",
    "                          dropout_p=0.5,\n",
    "                          stem_kernel_size=7\n",
    "                          ).to(DEVICE)\n",
    "\n",
    "summary(\n",
    "    model,\n",
    "    input_size=(3, 224, 224)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qGRNEWtzNEvN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5717012,
     "status": "ok",
     "timestamp": 1752334532713,
     "user": {
      "displayName": "Davide Ligabue",
      "userId": "11892369227096187309"
     },
     "user_tz": -120
    },
    "id": "qGRNEWtzNEvN",
    "outputId": "5f496bf2-70fc-4be6-dc4f-06bd47f62910"
   },
   "outputs": [],
   "source": [
    "final_config = {\"experiment_name\": \"Final\", \"n_layers\": 4, \"stem_kernel_size\": 7, \"use_batchnorm\": True, \"dropout_p\": 0.5, \"weight_decay\": 1e-3, \"use_scheduler\": True}\n",
    "results = run_experiment(final_config)\n",
    "\n",
    "plot_history(results[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192fb2ce",
   "metadata": {
    "id": "192fb2ce"
   },
   "source": [
    "## Part 2: fine-tune an existing network\n",
    "\n",
    "Your goal is to fine-tune a pretrained ResNet-18 model on `OxfordPetDataset`. Use the implementation provided by PyTorch, i.e. the opposite of part 1. Specifically, use the PyTorch ResNet-18 model pretrained on ImageNet-1K (V1). Divide your fine-tuning into two parts:\n",
    "\n",
    "2A. First, fine-tune the ResNet-18 with the same training hyperparameters you used for your best model in part 1.\n",
    "\n",
    "2B. Then, tweak the training hyperparameters in order to increase the accuracy on the test split. Justify your choices by analyzing the training plots and/or citing sources that guided you in your decisions â€” papers, blog posts, YouTube videos, or whatever else you may find useful. You should consider yourselves satisfied once you obtain a classification accuracy on the test split of ~90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef453b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "        num_classes: int,\n",
    "        weights: Optional[ResNet18_Weights] = None,\n",
    "        dropout_prob: Optional[float] = None\n",
    "    ) -> nn.Module:\n",
    "    \"\"\"Gets an image classifier based on ResNet-18.\n",
    "\n",
    "    Args:\n",
    "        num_classes: the number of classes.\n",
    "        weights: pretrained weights to load into the network.\n",
    "                 If None, the network is randomly initialized.\n",
    "\n",
    "    Returns:\n",
    "        The required network.\n",
    "    \"\"\"\n",
    "    model = resnet18(weights=weights)\n",
    "    # Here we override the old classifier\n",
    "    if dropout_prob is not None:\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(model.fc.in_features, num_classes)\n",
    "        )\n",
    "    else:\n",
    "      model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "def set_requires_grad(layers: list[torch.nn.Module], train: bool) -> None:\n",
    "    \"\"\"Sets the requires_grad attribute to True or False for each parameter within a layer.\n",
    "\n",
    "        Args:\n",
    "            layer: the layer to freeze.\n",
    "            train: if true train the layer.\n",
    "    \"\"\"\n",
    "    for layer in layers:\n",
    "      for p in layer.parameters():\n",
    "          p.requires_grad = train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9a7ae",
   "metadata": {},
   "source": [
    "### 2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "net_transfer = get_model(NUM_CLASSES, weights, dropout_prob=0.5).to(DEVICE)\n",
    "\n",
    "set_requires_grad([net_transfer.conv1,  net_transfer.bn1,\n",
    "                   net_transfer.layer1, net_transfer.layer2,\n",
    "                   net_transfer.layer3, net_transfer.layer4], True)\n",
    "\n",
    "summary(\n",
    "    net_transfer,\n",
    "    input_size=(3, 224, 224)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "transfer_config = {\"experiment_name\": \"Resnet transfer\", \"weight_decay\": 1e-3, \"use_scheduler\": True}\n",
    "results = run_experiment(transfer_config, net_transfer)\n",
    "\n",
    "plot_history(results[\"history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0243f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = torch.load(f\"Resnet transfer.pt\")\n",
    "net_transfer.load_state_dict(sd)\n",
    "\n",
    "set_requires_grad([net_transfer.conv1,  net_transfer.bn1,\n",
    "                   net_transfer.layer1, net_transfer.layer2,\n",
    "                   net_transfer.layer3, net_transfer.layer4], True)\n",
    "\n",
    "       \n",
    "EPOCHS = 25\n",
    "\n",
    "tuning_config = {\"experiment_name\": \"Resnet fine-tune\", \"lr\": 1e-4, \"weight_decay\": 1e-3, \"use_scheduler\": True}\n",
    "results = run_experiment(tuning_config, net_transfer)\n",
    "\n",
    "plot_history(results[\"history\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b83b7",
   "metadata": {},
   "source": [
    "As we can see, fine-tuning a ResNet18 model pretrained on ImageNet1k is able to achieve much better accuracy than a model trained from scratch, while also requiring much less memory and time to train.  \n",
    "Even with no hyperparameter changes, simply using the previous ones, the model already achieves an 84.18\\% test accuracy after the transfer learning phase, where the classifier is trained while freezing the feature extractor(the convolutional layers).  \n",
    "\n",
    "Fine-tuning of the entire model does not seem to be very effective, achieving an 86.62\\% test accuracy. The validation loss remains constant while the model starts to overfit, this suggest that the pretrained feature extractor is already close to optimal, at least with this particular choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819047dc",
   "metadata": {},
   "source": [
    "### 2B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47e094",
   "metadata": {},
   "source": [
    "To achieve the target accuracy of $\\sim 90 \\%$ our strategy was based on a two-phase training process (discussed during lectures):\n",
    "\n",
    "- Feature extraction: initially, all the convolutional layers were frozen and only the final, randomly initialized, classifier was trained for $50$ epoch, using a relatively high learning rate of $10^{-3}$,\n",
    "- Fine-tuning: The entire network was unfrozen and training was continued for a few epochs ($25$ epochs) with a much lower learning rate of $10^{-5}$ (it's quite important to use a small lr in order to avoid 'forgetting' the valuable pre-trained weights).\n",
    "\n",
    "Then to optimize the training process and improve the performance of the model two training choices were made that differ from the previous recipe:\n",
    "- AdamW was chosen as the optimizer. It's often better than the standard Adam optimizer due to the way it decouples weight decay from the learning rate (shown in this paper: 'Decoupled Weight Decay Regularization; Loshchilov & Hutter, 2019'),\n",
    "- The OneCycleLR was chosen as a scheduler. It's a powerful lr scheduler (as seen also during lectures) which accelerates convergence but also acts as a form of regularization (with a form of initial warm-up phase).\n",
    "\n",
    "Finally standard data preprocessing for ResNet was implemented, such as normalizing with ImageNet mean and variance and applying simple augmentations like RandomResizedCrop and RandomHorizontalFlip to make training more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97918997",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_image_net = [0.485, 0.456, 0.406]\n",
    "std_image_net = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_image_net, std_image_net)\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_image_net, std_image_net)\n",
    "])\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = OxfordPetDataset(split=\"train\", transform=train_transform)\n",
    "val_dataset = OxfordPetDataset(split=\"val\", transform=val_test_transform)\n",
    "test_dataset = OxfordPetDataset(split=\"test\", transform=val_test_transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2) # MAYBE ADD PIN MEMORY TRUE\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Get number of classes\n",
    "num_classes = train_dataset.get_num_classes()\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c83a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config: Dict, model = None):\n",
    "    print(\"=\"*60 + f\"\\nStart running the model: {config['experiment_name']}\\n\" + \"=\"*60)\n",
    "    fix_random(42)\n",
    "\n",
    "    # Setup of Early stopping, LR schedule and weight decay\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "    num_steps = EPOCHS * len(train_loader)\n",
    "    scheduler = lr_scheduler.OneCycleLR(optimizer, config[\"lr\"], total_steps=num_steps)\n",
    "\n",
    "    # Training Loop\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_val_acc = 0.0\n",
    "    pbar = tqdm(range(EPOCHS), desc=f\"Training {config['experiment_name']}\")\n",
    "    for _ in pbar:\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE, scheduler)\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, DEVICE)\n",
    "        history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n",
    "\n",
    "        if val_acc > best_val_acc: \n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f\"{config['experiment_name']}.pt\")\n",
    "            #print(f\"Saved new best model, val acc: {val_acc}\")\n",
    "        pbar.set_postfix({\"Val Acc\": f\"{val_acc:.4f}\", \"Train Acc\": f\"{train_acc:.4f}\"})\n",
    "\n",
    "    # Final test and save result\n",
    "    sd = torch.load(f\"{config['experiment_name']}.pt\")\n",
    "    model.load_state_dict(sd)\n",
    "    _, test_acc = evaluate_model(model, test_loader, criterion, DEVICE)\n",
    "    print(f\"Model '{config['experiment_name']}' completed. Test Accuracy: {test_acc:.4f}\\n\")\n",
    "\n",
    "    result = config.copy()\n",
    "    result.update({'test_accuracy': test_acc, 'best_val_accuracy': best_val_acc, 'history': history})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f901b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "net_2b_transfer = get_model(num_classes, weights).to(DEVICE)\n",
    "\n",
    "set_requires_grad([net_2b_transfer.conv1,  net_2b_transfer.bn1,\n",
    "                   net_2b_transfer.layer1, net_2b_transfer.layer2,\n",
    "                   net_2b_transfer.layer3, net_2b_transfer.layer4], False)\n",
    "\n",
    "summary(net_2b_transfer, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcce286",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "transfer_2b_config = {\"experiment_name\": \"Resnet 2B transfer\", \"lr\": 1e-3, \"weight_decay\": 1e-4}\n",
    "results = run_experiment(transfer_2b_config, net_2b_transfer)\n",
    "\n",
    "plot_history(results[\"history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02bbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = torch.load(\"Resnet 2B transfer.pt\")\n",
    "net_2b_transfer.load_state_dict(sd)\n",
    "\n",
    "set_requires_grad([net_2b_transfer.conv1,  net_2b_transfer.bn1,\n",
    "                   net_2b_transfer.layer1, net_2b_transfer.layer2,\n",
    "                   net_2b_transfer.layer3, net_2b_transfer.layer4], True)\n",
    "\n",
    "transfer_2b_config = {\"experiment_name\": \"Resnet 2B fine-tune\", \"lr\": 1e-5, \"weight_decay\": 1e-4}\n",
    "\n",
    "EPOCHS = 25\n",
    "results = run_experiment(transfer_2b_config, net_2b_transfer)\n",
    "plot_history(results[\"history\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a67db",
   "metadata": {},
   "source": [
    "As one can see, this model achieves a significant improvement over the previous one. 89.98\\% vs 86.62\\%.  \n",
    "The more apparent difference is actually the smoothness of the loss and accuracy history for the transfer learning phase. This is mainly due to the much gentler preprocessing transforms(`RandomResizedCrop` and `RandomHorizontalFlip` vs `RandomAugmentWide`) which in turns makes dropout unnecessary and weight decay less important, since the now unaugmented input data doesn't require such strong regularization anymore.\n",
    "\n",
    "The fine-tuning phase is also effective. Although the model starts to overfit a bit, validation loss decreased consistently unlike in the previous model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1XSMVo_3aaPQFs2DTel1DbdC_elmyIrcT",
     "timestamp": 1752328229862
    },
    {
     "file_id": "1KlfMQ80mz0C5VaHl9NJneJ1Kd5Fy_wCr",
     "timestamp": 1752325548141
    },
    {
     "file_id": "1LnonfVCkBTpYGiZ4PsV_bNo538IDeacc",
     "timestamp": 1752313785540
    },
    {
     "file_id": "1AyVE2BN9tv0YNC-fLGjbKCJc2pCszQpv",
     "timestamp": 1752310900810
    }
   ]
  },
  "kernelspec": {
   "display_name": "ipcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "462cccdddece4767b6780d5cda70b386": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ebbdf9d14b3474da7a106cbf3d47029": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b50a3ddb0908475297792e67f2c8e82b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c1b8ec3868be4727ada3ebf9ff70193c",
      "value": "Trainingâ€‡Noâ€‡BatchNorm:â€‡â€‡â€‡0%"
     }
    },
    "596c2d66539b4cfd89dee416ab7152fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67fc32d31aa74c1186b42ab1a0f09b3e",
      "max": 200,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_602e8996a79c4592928f043cf1a7a155",
      "value": 0
     }
    },
    "5bb6a216519842cdbaa740e786dfe74d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "602e8996a79c4592928f043cf1a7a155": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "67fc32d31aa74c1186b42ab1a0f09b3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "716c6c19df33489f8f170ab7d94b47f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4ebbdf9d14b3474da7a106cbf3d47029",
       "IPY_MODEL_596c2d66539b4cfd89dee416ab7152fa",
       "IPY_MODEL_b062e13508974a5e8a06366f24f1094b"
      ],
      "layout": "IPY_MODEL_462cccdddece4767b6780d5cda70b386"
     }
    },
    "7a4795c70250490882ef10c85fb07d65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b062e13508974a5e8a06366f24f1094b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a4795c70250490882ef10c85fb07d65",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5bb6a216519842cdbaa740e786dfe74d",
      "value": "â€‡0/200â€‡[00:00&lt;?,â€‡?it/s]"
     }
    },
    "b50a3ddb0908475297792e67f2c8e82b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1b8ec3868be4727ada3ebf9ff70193c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
